# Neural-Machine-Translation-NMT

Have you ever been to place and faced a difficulty of not being able to comunicate with people spaeking some different language? 
Keeping in mind that there are more than 7000 languages in this world, it is pretty natural that we need a human language translator. But what's the fun if we can't reduce the human effort. Neural networks might be complicated, but they have made our life pretty easy.

Neural Machine translation is the use of neural networks to convert one language to another when trained on a given dataset (paralled corpus for this model). In this model, I have used the English to Hindi corpus. The dataset can be refered from : [Parallel Corpus](http://www.manythings.org/anki/). The model is build on the keras framework. I have used LSTM in this model for training.

A little glimpse over LSTM, and encoder-decoder - LSTM model can be interpretted with a very simple example: 'I live in India, I speak Hindi'. If we apply normal feed-foward neural network and in the process of training when we feed 'speak' to our network, it will have to predict the output entirely on the basis of 'speak' alone which might turn out to be wrong. Wouldn't it be better if the network could retain the previous information so that it can be a relationship that whenever 'India' comes, the answer must be 'Hindi'. LSTM works on this principle only, it retains certain important information and forgets the rest of it. We use encoder-decoder model for NMT. In this model, we have two LSTMs, one for enocoder and the other for decoder. The work of the enocoder is to genarte a thought vector at the end, and the decoder takes this vector as an input and decodes the sentences into the other language.

After exploring the data, the first step is to *__clean__* the data. Convert the data into a proper dataframe. Checked if there are any null values and to remove them if present. Data preprocessing differs for each dataset. The main steps involve removing the punctuations, removing the quotes present in between the text, removing the extra spaces at the start or at the end of the string. According to this dataset, digits need to be removed from both the languages as both of them have different set of digits. The final thing is to add a *START_* and *_END* token to each of the string. The reason for this will be discussed later.

To convert these strings into data that can be feeded to the model, we need to split them into words and convert these words into vectors. For this, we calculate- total number of  unique words(*vocab*) present in both of the languages, maximum length of sentence(in terms of words) of both languages. We need to create two dictionaries. One should contain word as the key and it's corresponding index in *vocab* and the other one which is completely reverse of this dictionary i.e. index as they key and word as the value. The first dictionary will be used in training the encoder- decoder model whereas the second dictionary will be used in the inference model (Testing). 

The data is split into input and target variables which is further split into training and testing set. __*Important point-*__: We need to build a model which can perform efficiently even on a big dataset. SInce we want to it compute the results faster, so we split the data into batches to improve the speed. So, here we define a generate_batch function. Now this function combines all the major concepts. Let's discuss that one by one: 
      1. The input, output and batch size are defined.
      2. We carry out this function inside a while loop. We need to define three major things - *encoder input data, decoder input data, decoder target data*. The encoder              input has dimensions of (batch size, max input length), decoder input has dimensions of (batch size, max output length). We use np.zeros to create them so that we            have rows equal to the number batch size, and columns equal to number of max input/output length. This will also fulfil the purpose of padding ( so the strings have          the same length). Similarly we create decoder target with np,zeros with dimensions (batch size, max output length, length vocab of output words). This will create a          3D array so that we can one hot encode the probability of the words.
      3. We run a loop in each of the batches and start to fill up all these arrays to prepare them for training. For encoder input, we replace the zeros in the array with            the index of the corresponding word in loop. So even if the length is short, we have zeros present in the array (Padding).
      4. For decoder input, we want to include the string from start token but before end token whereas for decoder target, we want the string to start after start token but          should include the token. This is because we follow here the *teacher forcing* method. It's like a teacher continuously correcting a student who is trying to recite          a line. When decoder input is given, it might give some random prediction, so instead of feeding the network this predicted input, we feed it the actual input, so            that it could learn better ont the data. 

After defining the hyperparameters, we define the layers for this input. The initial input layer, the embedding layer (it keeps a check on the relationship among different words and accordingly passes the output), the main LSTM layer. After this layer we get three inputs - the cell state, the hidden state, and the output (hidden and cell state were intialized randomly). But we need to generate a thought vector at the end of this encoder model, so when the next input word id fed to the layer, it will use previous iteration's hidden state and cell state. SO after the last word is fed, we get a final hidden state and cell state. This is out *thought vector*. This thought vector needs to be given as a input the the decoder model.

For decoder LSTM, we define input layer, embedding layer, LSTM layer. But here instead of storing the hidden and cell state, we store the output. The Dense layer is for probability mapping of the word. After training the model, we build the inference model.

The inference model differs a bit from the training model. We define the input for hidden and cell state ( this we will get from encoder model), because that's what we are going to feed the network. After every word, the hidden and cell state will be passed for prediction of the next word. We intialize an array with Start in it(because that has always been our first input). Define a seperate encoder model. Feel this model with our test sentence. While a loop runs , the states values will be passed to predict output on this inferernce model. The value predicted will be an integer. Here we will use our previously created reverse dictionary so as to map the integer to the corresponding word. We will break from this loop once we have crossed a certain length or we have encountered *End* token. We keep on updating the states value and hence predict the results :)



